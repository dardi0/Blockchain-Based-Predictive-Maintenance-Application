# -*- coding: utf-8 -*-
"""
ğŸ¤– Training Utilities - ModÃ¼ler EÄŸitim FonksiyonlarÄ±
==================================================
Bu modÃ¼l train_model fonksiyonundan ayrÄ±ÅŸtÄ±rÄ±lan eÄŸitim fonksiyonlarÄ±nÄ± iÃ§erir.
Her fonksiyon tek bir sorumluluÄŸa odaklanÄ±r (Single Responsibility Principle).
"""

# --- MODÃœLER LSTM-CNN EÄÄ°TÄ°M UTILS ---

import pandas as pd
import numpy as np
import time
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, average_precision_score, matthews_corrcoef
from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import joblib
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUI olmadan Ã§alÄ±ÅŸmasÄ± iÃ§in

# KonfigÃ¼rasyon
from config import (
    FilePaths, ModelConfig, TrainingConfig, 
    LogConfig
)

# Genetik Algoritma kaldÄ±rÄ±ldÄ±

# TensorFlow warning'lerini bastÄ±r
LogConfig.suppress_tf_after_import()

class ModelTrainer:
    """
    ğŸš€ LSTM-CNN Model EÄŸitim SÄ±nÄ±fÄ±
    ===============================
    TÃ¼m eÄŸitim sÃ¼recini yÃ¶neten merkezi sÄ±nÄ±f
    """
    
    def __init__(self, config=None):
        """ModelTrainer sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r.

        Args:
            config (dict, optional): Ã–zel konfigÃ¼rasyon parametreleri. 
                                   VarsayÄ±lan deÄŸer None'dÄ±r.
        
        Example:
            >>> trainer = ModelTrainer()
        """
        self.config = config or {}
        
        # Durumlar (State)
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.optimal_threshold = TrainingConfig.DEFAULT_THRESHOLD
        self.results = {}
        
        print("ğŸ“Š Klasik Cross Validation modu aktif!")
        
        # EÄŸitim verileri
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
        print(f"ğŸ—ï¸ ModelTrainer baÅŸlatÄ±ldÄ±")
    
    def load_data(self):
        """AI4I2020 dataset'ini yÃ¼kler ve eÄŸitim/test setlerini hazÄ±rlar.

        Dataset'i yÃ¼kler, feature engineering yapar (Type deÄŸiÅŸkenini one-hot encode eder),
        hedef deÄŸiÅŸkeni ayÄ±rÄ±r ve train/test split iÅŸlemini gerÃ§ekleÅŸtirir.

        Returns:
            ModelTrainer: Method chaining iÃ§in kendisini dÃ¶ndÃ¼rÃ¼r.

        Raises:
            FileNotFoundError: Dataset dosyasÄ± bulunamazsa.
            ValueError: Dataset'te gerekli kolonlar yoksa.

        Example:
            >>> trainer = ModelTrainer()
            >>> trainer.load_data()
            ğŸ“ Veri yÃ¼kleme ve Ã¶n iÅŸleme baÅŸlÄ±yor...
            âœ… Veri hazÄ±rlandÄ±: EÄŸitim seti: (8000, 8)
        """
        print("ğŸ“ Veri yÃ¼kleme ve Ã¶n iÅŸleme baÅŸlÄ±yor...")
        
        try:
            # Dataset'i yÃ¼kle
            data = pd.read_csv(FilePaths.DATASET_PATH)
            print(f"ğŸ“Š Dataset yÃ¼klendi: {data.shape}")
            
            # DroplarÄ± kaldÄ±r (config'ten)
            data = data.drop(columns=TrainingConfig.DROP_COLUMNS, errors='ignore')
            
            # Type deÄŸiÅŸkenini one-hot encode et
            data = pd.get_dummies(data, columns=['Type'], prefix='Type')
            
            # Feature'larÄ± ve target'Ä± ayÄ±r
            self.feature_names = [col for col in data.columns if col != 'Machine failure']
            X = data[self.feature_names]
            y = data['Machine failure']
            
            print(f"ğŸ“‹ Features: {len(self.feature_names)} adet")
            print(f"ğŸ“Š Target daÄŸÄ±lÄ±mÄ±: {y.value_counts().to_dict()}")
            
            # Train/test split
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X, y,
                test_size=TrainingConfig.TEST_SIZE,
                random_state=TrainingConfig.TRAIN_RANDOM_STATE,
                stratify=y if TrainingConfig.STRATIFY else None
            )
            
            print(f"âœ… Veri hazÄ±rlandÄ±: EÄŸitim seti: {self.X_train.shape}, Test seti: {self.X_test.shape}")
            
            return self
            
        except Exception as e:
            print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
            raise

# training_utils.py dosyasÄ±ndaki run_cv fonksiyonunun tamamÄ±

    def run_cv(self):
        """5-Fold Cross Validation Ã§alÄ±ÅŸtÄ±rÄ±r ve sonuÃ§larÄ± doÄŸru formatta yapÄ±landÄ±rÄ±r."""
        print("\nğŸ”„ 5-Fold Cross Validation baÅŸlÄ±yor...")
    
        cv_start_time = time.time()
    
        skf = StratifiedKFold(n_splits=TrainingConfig.CV_SPLITS, shuffle=True, random_state=TrainingConfig.CV_RANDOM_STATE)
    
        # Her fold'dan gelen metrik sÃ¶zlÃ¼klerini saklamak iÃ§in bir liste
        fold_metric_list = []
        oof_true_list = []
        oof_pred_list = []
    
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X_train, self.y_train), 1):
            print(f"ğŸ“‹ Fold {fold}/{TrainingConfig.CV_SPLITS} iÅŸleniyor...")
        
            X_train_fold, X_val_fold = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]
            y_train_fold, y_val_fold = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]
        
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_fold)
            X_val_scaled = scaler.transform(X_val_fold)
        
            # SMOTE uygula (sadece train verisine)
            if TrainingConfig.USE_SMOTE:
                print(f"   ğŸ”„ SMOTE uygulanÄ±yor... (Ã–nce: {len(X_train_scaled)} Ã¶rnek)")
                smote = SMOTE(
                    random_state=TrainingConfig.SMOTE_RANDOM_STATE,
                    k_neighbors=TrainingConfig.SMOTE_K_NEIGHBORS
                )
                X_train_scaled, y_train_fold = smote.fit_resample(X_train_scaled, y_train_fold)
                print(f"   âœ… SMOTE tamamlandÄ± (Sonra: {len(X_train_scaled)} Ã¶rnek)")
        
            X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
            X_val_reshaped = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)
        
            model = self._create_model(input_shape=(X_train_scaled.shape[1], 1))
        
            early_stopping = EarlyStopping(
                monitor=TrainingConfig.EARLY_STOPPING_MONITOR,
                patience=TrainingConfig.EARLY_STOPPING_PATIENCE,
                restore_best_weights=TrainingConfig.EARLY_STOPPING_RESTORE_BEST,
                verbose=1
            )
        
            model.fit(
                X_train_reshaped, y_train_fold,
                epochs=TrainingConfig.EPOCHS,
                batch_size=TrainingConfig.BATCH_SIZE,
                validation_data=(X_val_reshaped, y_val_fold),
                callbacks=[early_stopping],
                verbose=1
            )
        
            y_pred_proba = model.predict(X_val_reshaped, verbose=0)
            try:
                oof_true_list.append(np.asarray(y_val_fold).ravel())
                oof_pred_list.append(np.asarray(y_pred_proba).ravel())
            except Exception:
                pass
        
            # Optimal eÅŸiÄŸi bul (yeni F-Beta yÃ¶ntemi)
            optimal_threshold = self._find_optimal_threshold(y_val_fold, y_pred_proba)

            # Config'ten varsayÄ±lan eÅŸik ve optimal eÅŸik ile tahminler yap
            y_pred_default = (y_pred_proba > TrainingConfig.DEFAULT_THRESHOLD).astype(int).flatten()
            y_pred_optimal = (y_pred_proba > optimal_threshold).astype(int).flatten()
        
            # Metrikleri her iki eÅŸik iÃ§in de hesapla
            metrics = self._calculate_metrics(y_val_fold, y_pred_default, y_pred_proba)
            metrics_opt = self._calculate_metrics(y_val_fold, y_pred_optimal, y_pred_proba)
        
            # Optimal metrikleri ana sÃ¶zlÃ¼ÄŸe ekle
            metrics['accuracy_opt'] = metrics_opt['accuracy']
            metrics['precision_opt'] = metrics_opt['precision']
            metrics['recall_opt'] = metrics_opt['recall']
            metrics['f1_opt'] = metrics_opt['f1']
            metrics['mcc_opt'] = metrics_opt['mcc']
            metrics['optimal_threshold'] = optimal_threshold
        
            fold_metric_list.append(metrics)
            
            # Confusion Matrix hesapla
            cm = confusion_matrix(y_val_fold, y_pred_optimal)
            tn, fp, fn, tp = cm.ravel()
            
            print(f"   âœ… Fold {fold}: Default F1={metrics['f1']:.4f}, Optimal F1={metrics['f1_opt']:.4f}, Recall={metrics['recall_opt']:.4f}")
            print(f"      ğŸ“Š Confusion Matrix: TP={tp}, FP={fp}, FN={fn}, TN={tn}")
            print(f"      ğŸ¯ ArÄ±za: {tp+fn} toplam, {tp} yakalandÄ±, {fn} kaÃ§Ä±rÄ±ldÄ±")

        # --- DEÄÄ°ÅÄ°KLÄ°K BURADA BAÅLIYOR ---
        # RaporlamanÄ±n beklediÄŸi doÄŸru veri yapÄ±sÄ±nÄ± (dict of lists) oluÅŸtur
        cv_results_structured = {
            'accuracy': [score['accuracy'] for score in fold_metric_list],
            'precision': [score['precision'] for score in fold_metric_list],
            'recall': [score['recall'] for score in fold_metric_list],
            'f1': [score['f1'] for score in fold_metric_list],
            'auc': [score['roc_auc'] for score in fold_metric_list],
            'mcc': [score['mcc'] for score in fold_metric_list],
            'entropy': [score.get('entropy') for score in fold_metric_list],
            'accuracy_opt': [score['accuracy_opt'] for score in fold_metric_list],
            'precision_opt': [score['precision_opt'] for score in fold_metric_list],
            'recall_opt': [score['recall_opt'] for score in fold_metric_list],
            'f1_opt': [score['f1_opt'] for score in fold_metric_list],
            'mcc_opt': [score['mcc_opt'] for score in fold_metric_list],
            'optimal_threshold': [score['optimal_threshold'] for score in fold_metric_list]
        }

        # Global tarama ile tek eÅŸik seÃ§imi (OOF tahminleri)
        try:
            oof_y = np.concatenate(oof_true_list) if len(oof_true_list) else np.array([])
            oof_p = np.concatenate(oof_pred_list) if len(oof_pred_list) else np.array([])
            if oof_y.size > 0 and oof_p.size > 0:
                best_global_th = self._find_optimal_threshold(oof_y, oof_p)
                self.optimal_threshold = float(best_global_th)
                print(f"\nSeÃ§ilen nihai optimal eÅŸik (Global PR tarama): {self.optimal_threshold:.3f}")
            else:
                self.optimal_threshold = float(np.mean(cv_results_structured['optimal_threshold']))
                print(f"\nOOF bulunamadÄ±; CV ortalama eÅŸiÄŸi kullanÄ±lÄ±yor: {self.optimal_threshold:.3f}")
        except Exception as e:
            self.optimal_threshold = TrainingConfig.DEFAULT_THRESHOLD
            print(f"\nGlobal eÅŸik belirlenemedi ({e}); varsayÄ±lan eÅŸik kullanÄ±lÄ±yor: {self.optimal_threshold:.3f}")
    
        cv_end_time = time.time()
    
        # SonuÃ§larÄ± ana sonuÃ§lar sÃ¶zlÃ¼ÄŸÃ¼ne kaydet
        self.results['cv_results'] = cv_results_structured
        self.results['cv_time'] = cv_end_time - cv_start_time

        print(f"âœ… Cross Validation tamamlandÄ±! (SÃ¼re: {self.results['cv_time']:.1f}s)")
        print(f"ğŸ“Š Ortalama F1 (Optimal EÅŸik): {np.mean(cv_results_structured['f1_opt']):.4f} Â± {np.std(cv_results_structured['f1_opt']):.4f}")
    
        return self

    def train_final(self):
        """Final model eÄŸitimi yapar"""
        print("\nğŸ¯ Final model eÄŸitimi baÅŸlÄ±yor...")
        
        final_start_time = time.time()
        
        # Scaler oluÅŸtur ve veriyi Ã¶lÃ§eklendir
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(self.X_train)
        X_test_scaled = self.scaler.transform(self.X_test)
        
        # SMOTE uygula (sadece train verisine)
        if TrainingConfig.USE_SMOTE:
            print(f"ğŸ”„ SMOTE uygulanÄ±yor... (Ã–nce: {len(X_train_scaled)} Ã¶rnek)")
            y_train_np = np.asarray(self.y_train)
            print(f"   ğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±: {np.bincount(y_train_np)}")
            smote = SMOTE(
                random_state=TrainingConfig.SMOTE_RANDOM_STATE,
                k_neighbors=TrainingConfig.SMOTE_K_NEIGHBORS
            )
            X_train_scaled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train_np)
            self.y_train = y_train_resampled  # numpy array olarak sakla
            print(f"âœ… SMOTE tamamlandÄ± (Sonra: {len(X_train_scaled)} Ã¶rnek)")
            print(f"   ğŸ“Š Yeni sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±: {np.bincount(self.y_train)}")
        
        # LSTM iÃ§in reshape
        X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
        X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)
        
        # Model oluÅŸtur
        self.model = self._create_model(input_shape=(X_train_scaled.shape[1], 1))
        
        # Early stopping
        early_stopping = EarlyStopping(
            monitor=TrainingConfig.EARLY_STOPPING_MONITOR,
            patience=TrainingConfig.FINAL_MODEL_PATIENCE,
            restore_best_weights=TrainingConfig.EARLY_STOPPING_RESTORE_BEST,
            verbose=1
        )
        
        # Model eÄŸitimi
        history = self.model.fit(
            X_train_reshaped, self.y_train,
            epochs=TrainingConfig.FINAL_MODEL_EPOCHS,
            batch_size=TrainingConfig.BATCH_SIZE,
            validation_split=TrainingConfig.VALIDATION_SPLIT,
            callbacks=[early_stopping],
            verbose=1
        )
        
        # Test tahminleri
        y_pred_prob = self.model.predict(X_test_reshaped, verbose=0)
        
        # Optimal threshold bulma (yeni F-Beta yÃ¶ntemi)
        print("\nğŸ¯ Test seti iÃ§in optimal eÅŸik bulunuyor...")
        best_threshold = self.optimal_threshold
        print(f"\nUsing CV-derived optimal threshold for test evaluation: {best_threshold:.3f}")
        
        
        
        # SonuÃ§larÄ± hesapla - Config'ten varsayÄ±lan eÅŸik
        y_pred_default = (y_pred_prob > TrainingConfig.DEFAULT_THRESHOLD).astype(int).flatten()
        y_pred_optimal = (y_pred_prob > best_threshold).astype(int).flatten()
        
        default_results = self._calculate_metrics(self.y_test, y_pred_default, y_pred_prob)
        optimal_results = self._calculate_metrics(self.y_test, y_pred_optimal, y_pred_prob)
        
        final_end_time = time.time()
        
        # Test sonuÃ§larÄ±nÄ± kaydet
        # Entropi Ã¶zeti (test seti)
        try:
            p_test = np.clip(np.array(y_pred_prob).flatten(), 1e-12, 1 - 1e-12)
            entropy_test_vals = -(p_test * np.log2(p_test) + (1 - p_test) * np.log2(1 - p_test))
            entropy_test_mean = float(np.mean(entropy_test_vals))
        except Exception:
            entropy_test_mean = float('nan')

        test_results = {
            'default_threshold_results': default_results,
            'optimal_threshold_results': optimal_results,
            'optimal_threshold': best_threshold,
            'training_time': final_end_time - final_start_time,
            'history': history.history,
            'y_true': self.y_test.values,
            'y_pred_proba': y_pred_prob.flatten(),
            'y_pred': y_pred_optimal.flatten(),
            'entropy_mean': entropy_test_mean,
        }
        
        self.results['test_results'] = test_results
        
        # Confusion Matrix hesapla
        cm_final = confusion_matrix(self.y_test, y_pred_optimal)
        tn_final, fp_final, fn_final, tp_final = cm_final.ravel()
        
        print(f"âœ… Nihai model eÄŸitimi tamamlandÄ±! (SÃ¼re: {test_results['training_time']:.1f}s)")
        print(f"ğŸ“Š Final Performans: Precision={optimal_results['precision']:.4f}, Recall={optimal_results['recall']:.4f}, F1={optimal_results['f1']:.4f}")
        print(f"\nğŸ“Š Confusion Matrix (Test Seti):")
        print(f"   TN={tn_final:4d}  FP={fp_final:4d}")
        print(f"   FN={fn_final:4d}  TP={tp_final:4d}")
        print(f"\nğŸ¯ Test Seti Analizi:")
        print(f"   Toplam arÄ±za: {tp_final + fn_final}")
        print(f"   Yakalanan arÄ±za: {tp_final}")
        print(f"   KaÃ§Ä±rÄ±lan arÄ±za (FN): {fn_final}")
        print(f"   YanlÄ±ÅŸ alarm (FP): {fp_final}")
        
        return self
    
    def run_training_pipeline(self):
        """
        Tam eÄŸitim pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r
        Returns: (model, scaler, results)
        """
        print("ğŸš€ Tam EÄŸitim Pipeline'Ä± BaÅŸlÄ±yor...")
        pipeline_start = time.time()
        
        # Klasik pipeline adÄ±mlarÄ±
        print("ğŸ“Š Klasik Cross Validation Modu Aktif!")
        self.load_data()
        self.run_cv()
        self.train_final()
        self.save_models()
        
        pipeline_end = time.time()
        total_time = pipeline_end - pipeline_start
        
        print(f"\nğŸ‰ EÄÄ°TÄ°M PÄ°PELÄ°NE'I TAMAMLANDI!")
        print(f"â±ï¸ Toplam SÃ¼re: {total_time:.1f} saniye")
        
        if 'test_results' in self.results:
            print(f"ğŸ¯ Final F1 Skoru: {self.results['test_results']['optimal_threshold_results']['f1']:.4f}")
            print(f"ğŸšï¸ Optimal EÅŸik: {self.optimal_threshold:.3f}")
        
        return self.model, self.scaler, self.results
    
    def save_models(self):
        """Model ve scaler'Ä± kaydeder"""
        try:
            # Model kaydet
            self.model.save(FilePaths.MODEL_PATH)
            print(f"ğŸ’¾ Model kaydedildi: {FilePaths.MODEL_PATH}")
            
            # Scaler kaydet
            joblib.dump(self.scaler, FilePaths.SCALER_PATH)
            print(f"ğŸ’¾ Scaler kaydedildi: {FilePaths.SCALER_PATH}")
            
            # Optimal threshold ve metadata kaydet
            metadata = {
                'optimal_threshold': float(self.optimal_threshold),
                'feature_names': self.feature_names,
                'model_version': '1.0',
                'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            metadata_path = FilePaths.MODEL_PATH.parent / 'model_metadata.pkl'
            joblib.dump(metadata, metadata_path)
            print(f"ğŸ’¾ Model metadata kaydedildi: {metadata_path}")
            print(f"   ğŸ¯ Optimal Threshold: {self.optimal_threshold:.3f}")
            
            return self
            
        except Exception as e:
            print(f"âŒ Model kaydetme hatasÄ±: {e}")
            raise

    def _create_model(self, input_shape):
        """Config'ten parametreleri alarak LSTM-CNN model oluÅŸturur"""

        model = Sequential()

       # config.py'deki CNN_LAYERS sayÄ±sÄ± kadar dÃ¶ngÃ¼ Ã§alÄ±ÅŸÄ±r.
        for i, filters in enumerate(ModelConfig.CNN_FILTERS_PER_LAYER):
            # Her katmanda filtre sayÄ±sÄ±nÄ± artÄ±rma seÃ§eneÄŸi 
            # filters = ModelConfig.CNN_FILTERS * (2**i) 
        
            if i == 0:
                # Ä°lk katman, input_shape'i belirtmek zorundadÄ±r.
                model.add(Conv1D(   
                    filters=filters, 
                    kernel_size=ModelConfig.CNN_KERNEL_SIZE, 
                    activation=ModelConfig.CNN_ACTIVATION, 
                    input_shape=input_shape, 
                    padding='same'
                ))
            else:
                # Sonraki katmanlar
                model.add(Conv1D(
                    filters=filters, 
                    kernel_size=ModelConfig.CNN_KERNEL_SIZE, 
                    activation=ModelConfig.CNN_ACTIVATION,
                    padding='same'
                ))
            
            model.add(MaxPooling1D(pool_size=ModelConfig.CNN_POOL_SIZE))
            model.add(Dropout(ModelConfig.CNN_DROPOUT))
    
        # config.py'deki LSTM_LAYERS sayÄ±sÄ± kadar dÃ¶ngÃ¼ Ã§alÄ±ÅŸÄ±r.
        num_lstm_layers = len(ModelConfig.LSTM_UNITS_PER_LAYER)
        for i, units in enumerate(ModelConfig.LSTM_UNITS_PER_LAYER):
            # Son LSTM katmanÄ± hariÃ§, bir sonraki LSTM katmanÄ±na veri aktarmak iÃ§in
            # return_sequences=True olmalÄ±dÄ±r.  
            return_sequences = (i < num_lstm_layers - 1)
        
            model.add(LSTM(
                units=units, 
                return_sequences=return_sequences, 
                dropout=ModelConfig.LSTM_DROPOUT
            ))
    
        # --- Dinamik Dense KatmanlarÄ± ---
        # config.py'deki DENSE_LAYERS sayÄ±sÄ± kadar dÃ¶ngÃ¼ Ã§alÄ±ÅŸÄ±r.
        for i, units in enumerate(ModelConfig.DENSE_UNITS_PER_LAYER):
        
            model.add(Dense(units, activation=ModelConfig.DENSE_ACTIVATION))
            model.add(Dropout(ModelConfig.DENSE_DROPOUT))
        
        # --- Ã‡Ä±kÄ±ÅŸ KatmanÄ± ---
        model.add(Dense(1, activation='sigmoid'))
    
        # --- Model Derleme ---
        # TÃ¼m parametreler artÄ±k config.py'den geliyor.
        try:
            required_precision = float(TrainingConfig.MIN_PRECISION_THRESHOLD)
        except Exception:
            required_precision = 0.5
        pr_auc_metric = tf.keras.metrics.AUC(curve='PR', name='pr_auc')
        roc_auc_metric = tf.keras.metrics.AUC(curve='ROC', name='roc_auc')
        recall_at_precision_metric = tf.keras.metrics.RecallAtPrecision(
            precision=required_precision,
            num_thresholds=200,
            name=f"recall_at_p{int(required_precision*100)}"
        )
        model.compile(
            optimizer=Adam(learning_rate=ModelConfig.LEARNING_RATE),
            loss=ModelConfig.LOSS_FUNCTION,
            metrics=[pr_auc_metric, roc_auc_metric, recall_at_precision_metric]
        )
        
        print("âœ… Dinamik model baÅŸarÄ±yla oluÅŸturuldu!")
        model.summary() # OluÅŸturulan modelin Ã¶zetini konsola yazdÄ±r
    
        return model
    
    def _calculate_metrics(self, y_true, y_pred, y_pred_proba):
        """Metrikleri hesaplar"""
        # Shannon entropisi (ikili sÄ±nÄ±flandÄ±rma) â€“ belirsizlik Ã¶lÃ§Ã¼mÃ¼ (bit)
        try:
            p = np.clip(np.array(y_pred_proba).flatten(), 1e-12, 1 - 1e-12)
            entropy_vals = -(p * np.log2(p) + (1 - p) * np.log2(1 - p))
            entropy_mean = float(np.mean(entropy_vals))
        except Exception:
            entropy_mean = float('nan')

        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_true, p),
            'mcc': matthews_corrcoef(y_true, y_pred),
            'entropy': entropy_mean,
        }
    
    def _find_optimal_threshold(self, y_true, y_pred_proba):
        """
        Optimal eÅŸiÄŸi bulur (konfigÃ¼re edilebilir yÃ¶ntemler)
        
        Args:
            y_true: GerÃ§ek etiketler
            y_pred_proba: Model olasÄ±lÄ±k tahminleri
            
        Returns:
            float: Optimal eÅŸik deÄŸeri
            
        Methods:
            - 'f1': F1-Score maksimizasyonu (varsayÄ±lan)
            - 'f_beta': F-Beta Score (Recall Ã¶ncelikli, beta=2.0)
            - 'recall_focused': Minimum precision kÄ±sÄ±tÄ± ile recall maksimizasyonu
        """
        p = np.asarray(y_pred_proba).ravel()
        precisions, recalls, thresholds = precision_recall_curve(y_true, p)
        prec = precisions[1:]
        rec = recalls[1:]
        
        method = TrainingConfig.THRESHOLD_OPTIMIZATION_METHOD
        
        if method == 'f1':
            # YÃ¶ntem 1: F1-Score maksimizasyonu (mevcut yÃ¶ntem)
            f1_scores = 2 * (prec * rec) / (prec + rec + 1e-8)
            best_idx = np.argmax(f1_scores)
            print(f"   â„¹ï¸ EÅŸik optimizasyon yÃ¶ntemi: F1-Score maksimizasyonu")
            
        elif method == 'f_beta':
            # YÃ¶ntem 2: F-Beta Score (Recall Ã¶ncelikli)
            beta = TrainingConfig.F_BETA_VALUE
            f_beta_scores = (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec + 1e-8)
            best_idx = np.argmax(f_beta_scores)
            print(f"   â„¹ï¸ EÅŸik optimizasyon yÃ¶ntemi: F-Beta Score (Î²={beta}) - Recall Ã¶ncelikli")
            
        elif method == 'recall_focused':
            # YÃ¶ntem 3: Minimum precision kÄ±sÄ±tÄ± ile recall maksimizasyonu
            min_precision = TrainingConfig.MIN_PRECISION_THRESHOLD
            valid_indices = np.where(prec >= min_precision)[0]
            
            if len(valid_indices) > 0:
                best_idx_local = np.argmax(rec[valid_indices])
                best_idx = valid_indices[best_idx_local]
                print(f"   â„¹ï¸ EÅŸik optimizasyon yÃ¶ntemi: Recall maksimizasyonu (Min Precision={min_precision})")
            else:
                # HiÃ§bir eÅŸik minimum precision'Ä± saÄŸlamÄ±yorsa, en yÃ¼ksek recall'Ä± seÃ§
                best_idx = np.argmax(rec)
                print(f"   âš ï¸ Min precision={min_precision} saÄŸlanamadÄ±, en yÃ¼ksek recall seÃ§ildi")
                
        else:
            # VarsayÄ±lan: F1-Score
            f1_scores = 2 * (prec * rec) / (prec + rec + 1e-8)
            best_idx = np.argmax(f1_scores)
            print(f"   â„¹ï¸ EÅŸik optimizasyon yÃ¶ntemi: F1-Score (varsayÄ±lan)")
        
        optimal_threshold = float(thresholds[int(best_idx)])
        optimal_precision = float(prec[int(best_idx)])
        optimal_recall = float(rec[int(best_idx)])
        
        print(f"   ğŸ“Š Optimal eÅŸik: {optimal_threshold:.3f} (Precision={optimal_precision:.3f}, Recall={optimal_recall:.3f})")
        
        return optimal_threshold

# Backward compatibility iÃ§in eski fonksiyonlar
def run_cross_validation(X_train, y_train):
    """Eski API uyumluluÄŸu iÃ§in wrapper"""
    trainer = ModelTrainer()
    trainer.X_train = X_train
    trainer.y_train = y_train
    trainer.run_cv()
    cv_results = trainer.results['cv_results']
    return (cv_results, cv_results['fold_predictions'], 
            cv_results['fold_probabilities'], cv_results['fold_true_labels'])

def train_final_model(X_train, y_train, X_test, y_test, cv_scores):
    """Eski API uyumluluÄŸu iÃ§in wrapper"""
    trainer = ModelTrainer()
    trainer.X_train = X_train
    trainer.y_train = y_train
    trainer.X_test = X_test
    trainer.y_test = y_test
    trainer.train_final()
    return trainer.model, trainer.scaler, trainer.optimal_threshold, trainer.results['test_results']
