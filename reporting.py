# -*- coding: utf-8 -*-
"""
ğŸ“ˆ Reporting & Visualization - Raporlama ve GÃ¶rselleÅŸtirme ModÃ¼lÃ¼
================================================================
Bu modÃ¼l train_model fonksiyonundan ayrÄ±ÅŸtÄ±rÄ±lan raporlama ve gÃ¶rselleÅŸtirme
fonksiyonlarÄ±nÄ± iÃ§erir. Tek sorumluluk: SonuÃ§larÄ± gÃ¶stermek.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
import os
from pathlib import Path
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, average_precision_score, matthews_corrcoef

# Performans metrikleri klasÃ¶rÃ¼nÃ¼ oluÅŸtur
METRICS_DIR = Path("Performans_Metrikleri")
METRICS_DIR.mkdir(exist_ok=True)

# Config'ten import'lar
from config import VisualizationConfig, TrainingConfig

def print_cv_results(cv_scores):
    """Cross Validation sonuÃ§larÄ±nÄ± detaylÄ± tablolarla konsola yazdÄ±rÄ±r.

    5-fold CV sonuÃ§larÄ±nÄ± hem varsayÄ±lan eÅŸik (0.5) hem de optimal eÅŸik iÃ§in
    formatlanmÄ±ÅŸ tablolar halinde gÃ¶sterir. Her metrik iÃ§in ortalama, standart
    sapma, minimum ve maksimum deÄŸerleri iÃ§erir.

    Args:
        cv_scores (dict): CV sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k. Åu anahtarlarÄ± iÃ§ermeli:
            - accuracy, precision, recall, f1, auc (list): Her fold iÃ§in metrikler
            - accuracy_opt, precision_opt, recall_opt, f1_opt (list): Optimal eÅŸik metrikleri
            - optimal_threshold (list): Her fold iÃ§in optimal eÅŸik deÄŸerleri

    Example:
        >>> cv_scores = {'accuracy': [0.8, 0.82, 0.81], 'f1': [0.75, 0.77, 0.76]}
        >>> print_cv_results(cv_scores)
        ================================================================================
        ğŸ¯ 5-FOLD CROSS VALIDATION SONUÃ‡LARI
        ================================================================================
        ğŸ“Š CROSS VALIDATION PERFORMANS METRÄ°KLERÄ° (0.5 EÅÄ°ÄÄ°):
        ...
    """
    print(f"\n{'='*80}")
    print(f"ğŸ¯ {TrainingConfig.CV_SPLITS}-FOLD CROSS VALIDATION SONUÃ‡LARI")
    print(f"{'='*80}")
    
    # CV performans metrikleri tablosu (config'ten varsayÄ±lan eÅŸik)
    print(f"\nğŸ“Š CROSS VALIDATION PERFORMANS METRÄ°KLERÄ° ({TrainingConfig.DEFAULT_THRESHOLD} EÅÄ°ÄÄ°):")
    print(f"â”Œ{'â”€'*20}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”")
    print(f"â”‚ {'Metrik':<18} â”‚ {'Ortalama':<8} â”‚ {'Std':<8} â”‚ {'Min':<8} â”‚ {'Max':<8} â”‚")
    print(f"â”œ{'â”€'*20}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¤")
    
    standard_metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'mcc', 'entropy']
    for metric_name in standard_metrics:
        scores = cv_scores[metric_name]
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        min_score = np.min(scores)
        max_score = np.max(scores)
        display_name = metric_name.capitalize().replace('Auc', 'AUC').replace('Mcc', 'MCC')
        if metric_name == 'entropy':
            display_name = 'Entropy'
        
        print(f"â”‚ {display_name:<18} â”‚ {mean_score:<8.4f} â”‚ {std_score:<8.4f} â”‚ {min_score:<8.4f} â”‚ {max_score:<8.4f} â”‚")
    
    print(f"â””{'â”€'*20}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”˜")
    
    # CV performans metrikleri tablosu (Optimal eÅŸiÄŸi)
    print(f"\nğŸ¯ CROSS VALIDATION PERFORMANS METRÄ°KLERÄ° (OPTÄ°MAL EÅÄ°K):")
    print(f"â”Œ{'â”€'*20}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”")
    print(f"â”‚ {'Metrik':<18} â”‚ {'Ortalama':<8} â”‚ {'Std':<8} â”‚ {'Min':<8} â”‚ {'Max':<8} â”‚")
    print(f"â”œ{'â”€'*20}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¤")
    
    optimal_metrics = ['accuracy_opt', 'precision_opt', 'recall_opt', 'f1_opt', 'mcc_opt', 'optimal_threshold']
    for metric_name in optimal_metrics:
        scores = cv_scores[metric_name]
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        min_score = np.min(scores)
        max_score = np.max(scores)
        
        if metric_name == 'optimal_threshold':
            display_name = 'Optimal Threshold'
        else:
            display_name = metric_name.replace('_opt', '').capitalize().replace('Mcc', 'MCC')
        
        print(f"â”‚ {display_name:<18} â”‚ {mean_score:<8.4f} â”‚ {std_score:<8.4f} â”‚ {min_score:<8.4f} â”‚ {max_score:<8.4f} â”‚")
    
    print(f"â””{'â”€'*20}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”˜")
    
    # Ä°yileÅŸme analizi - TÃ¼m metrikler iÃ§in
    accuracy_improvement = ((np.mean(cv_scores['accuracy_opt']) - np.mean(cv_scores['accuracy'])) / np.mean(cv_scores['accuracy'])) * 100
    precision_improvement = ((np.mean(cv_scores['precision_opt']) - np.mean(cv_scores['precision'])) / np.mean(cv_scores['precision'])) * 100
    recall_improvement = ((np.mean(cv_scores['recall_opt']) - np.mean(cv_scores['recall'])) / np.mean(cv_scores['recall'])) * 100
    f1_improvement = ((np.mean(cv_scores['f1_opt']) - np.mean(cv_scores['f1'])) / np.mean(cv_scores['f1'])) * 100
    mcc_improvement = ((np.mean(cv_scores['mcc_opt']) - np.mean(cv_scores['mcc'])) / abs(np.mean(cv_scores['mcc'])) * 100 if np.mean(cv_scores['mcc']) != 0 else 0)
    
    print(f"\nâš¡ OPTÄ°MAL EÅÄ°K Ä°YÄ°LEÅME ANALÄ°ZÄ°:")
    print(f"   â€¢ Accuracy Ä°yileÅŸme: %{accuracy_improvement:+.2f}")
    print(f"   â€¢ Precision Ä°yileÅŸme: %{precision_improvement:+.2f}")
    print(f"   â€¢ Recall Ä°yileÅŸme: %{recall_improvement:+.2f}")
    print(f"   â€¢ F1-Score Ä°yileÅŸme: %{f1_improvement:+.2f}")
    print(f"   â€¢ MCC Ä°yileÅŸme: %{mcc_improvement:+.2f}")
    print(f"   â€¢ Ortalama Optimal EÅŸik: {np.mean(cv_scores['optimal_threshold']):.3f}")
    print(f"   â€¢ EÅŸik Standart Sapma: {np.std(cv_scores['optimal_threshold']):.3f}")
    
    # Her fold iÃ§in detay (config'ten varsayÄ±lan eÅŸik)
    print(f"\nğŸ“‹ FOLD DETAYLARI ({TrainingConfig.DEFAULT_THRESHOLD} EÅÄ°ÄÄ°):")
    print(f"â”Œ{'â”€'*6}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”")
    print(f"â”‚ {'Fold':<4} â”‚ {'Accuracy':<8} â”‚ {'Precision':<8} â”‚ {'Recall':<8} â”‚ {'F1':<8} â”‚ {'AUC':<8} â”‚ {'MCC':<8} â”‚")
    print(f"â”œ{'â”€'*6}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¤")
    
    for i in range(TrainingConfig.CV_SPLITS):
        print(f"â”‚ {i+1:<4} â”‚ {cv_scores['accuracy'][i]:<8.4f} â”‚ {cv_scores['precision'][i]:<8.4f} â”‚ {cv_scores['recall'][i]:<8.4f} â”‚ {cv_scores['f1'][i]:<8.4f} â”‚ {cv_scores['auc'][i]:<8.4f} â”‚ {cv_scores['mcc'][i]:<8.4f} â”‚")
    
    print(f"â””{'â”€'*6}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”˜")
    
    # Her fold iÃ§in detay (Optimal eÅŸiÄŸi)
    print(f"\nğŸ¯ FOLD DETAYLARI (OPTÄ°MAL EÅÄ°K):")
    print(f"â”Œ{'â”€'*6}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”")
    print(f"â”‚ {'Fold':<4} â”‚ {'Accuracy':<8} â”‚ {'Precision':<8} â”‚ {'Recall':<8} â”‚ {'F1':<8} â”‚ {'MCC':<8} â”‚ {'EÅŸik':<8} â”‚")
    print(f"â”œ{'â”€'*6}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¤")
    
    for i in range(TrainingConfig.CV_SPLITS):
        print(f"â”‚ {i+1:<4} â”‚ {cv_scores['accuracy_opt'][i]:<8.4f} â”‚ {cv_scores['precision_opt'][i]:<8.4f} â”‚ {cv_scores['recall_opt'][i]:<8.4f} â”‚ {cv_scores['f1_opt'][i]:<8.4f} â”‚ {cv_scores['mcc_opt'][i]:<8.4f} â”‚ {cv_scores['optimal_threshold'][i]:<8.3f} â”‚")
    
    print(f"â””{'â”€'*6}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”˜")

def print_test_results(test_results, cv_scores):
    """Test seti performans sonuÃ§larÄ±nÄ± detaylÄ± tablolarla konsola yazdÄ±rÄ±r.

    Final modelin test seti Ã¼zerindeki performansÄ±nÄ± hem varsayÄ±lan eÅŸik (0.5)
    hem de optimal eÅŸik ile deÄŸerlendirir. CV sonuÃ§larÄ± ile karÅŸÄ±laÅŸtÄ±rma yapar.

    Args:
        test_results (dict): Test sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k. Åu anahtarlarÄ± iÃ§ermeli:
            - y_true (array): GerÃ§ek test etiketleri
            - y_pred (array): Optimal eÅŸik uygulanmÄ±ÅŸ test tahminleri
            - y_pred_proba (array): Test olasÄ±lÄ±klarÄ±
            - optimal_threshold (float): KullanÄ±lan optimal eÅŸik deÄŸeri
        cv_scores (dict): CV sonuÃ§larÄ± (karÅŸÄ±laÅŸtÄ±rma iÃ§in).

    Example:
        >>> print_test_results(test_results, cv_scores)
        ================================================================================
        ğŸ¯ FINAL MODEL TEST SETÄ° SONUÃ‡LARI
        ================================================================================
        ğŸ“Š TEST SETÄ° PERFORMANS METRÄ°KLERÄ° (0.5 EÅÄ°ÄÄ°):
        ...
    """
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    y_pred_opt = test_results['y_pred']  # Bu optimal threshold ile tahmin edilmiÅŸ
    optimal_threshold = test_results['optimal_threshold']
    
    # Normal threshold (config'ten) ile tahmin hesapla
    y_pred = (y_pred_prob > TrainingConfig.DEFAULT_THRESHOLD).astype(int)
    
    # DetaylÄ± performans metrikleri (0.5 eÅŸiÄŸi)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_precision = precision_score(y_test, y_pred, zero_division=0)
    test_recall = recall_score(y_test, y_pred, zero_division=0)
    test_f1 = f1_score(y_test, y_pred, zero_division=0)
    test_auc = roc_auc_score(y_test, y_pred_prob)
    test_auc_pr = average_precision_score(y_test, y_pred_prob)  # AUC-PR (Average Precision)
    test_mcc = matthews_corrcoef(y_test, y_pred)  # Matthews Correlation Coefficient
    # Entropi (Shannon, bit) â€“ model belirsizliÄŸi (ortalama)
    try:
        p_entropy = np.clip(np.array(y_pred_prob).flatten(), 1e-12, 1 - 1e-12)
        test_entropy = float(np.mean(-(p_entropy * np.log2(p_entropy) + (1 - p_entropy) * np.log2(1 - p_entropy))))
    except Exception:
        test_entropy = float('nan')
    
    # DetaylÄ± performans metrikleri (Optimal eÅŸiÄŸi)
    test_accuracy_opt = accuracy_score(y_test, y_pred_opt)
    test_precision_opt = precision_score(y_test, y_pred_opt, zero_division=0)
    test_recall_opt = recall_score(y_test, y_pred_opt, zero_division=0)
    test_f1_opt = f1_score(y_test, y_pred_opt, zero_division=0)
    test_mcc_opt = matthews_corrcoef(y_test, y_pred_opt)  # MCC optimal eÅŸik
    
    # Confusion Matrix (0.5 eÅŸiÄŸi)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # Ek: Confusion Matrix (Optimal eÅŸik) - metin olarak ek gÃ¶sterim
    cm_opt = confusion_matrix(y_test, y_pred_opt)
    tn_o, fp_o, fn_o, tp_o = cm_opt.ravel()
    print(f"\nTEST SETÄ° CONFUSION MATRIX (OPTÄ°MAL EÅÄ°K: {optimal_threshold:.3f}):")
    print(f"TN={tn_o}  FP={fp_o}")
    print(f"FN={fn_o}  TP={tp_o}")
    
    print(f"\nğŸ¯ NÄ°HAÄ° LSTM-CNN MODEL TEST PERFORMANSI ({TrainingConfig.DEFAULT_THRESHOLD} EÅÄ°ÄÄ°):")
    print(f"â”Œ{'â”€'*25}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*15}â”")
    print(f"â”‚ {'Metrik':<23} â”‚ {'Test':<8} â”‚ {'CV Ort.':<8} â”‚ {'Fark':<13} â”‚")
    print(f"â”œ{'â”€'*25}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*15}â”¤")
    print(f"â”‚ {'DoÄŸruluk (Accuracy)':<23} â”‚ {test_accuracy:<8.4f} â”‚ {np.mean(cv_scores['accuracy']):<8.4f} â”‚ {test_accuracy - np.mean(cv_scores['accuracy']):+8.4f} â”‚")
    print(f"â”‚ {'Kesinlik (Precision)':<23} â”‚ {test_precision:<8.4f} â”‚ {np.mean(cv_scores['precision']):<8.4f} â”‚ {test_precision - np.mean(cv_scores['precision']):+8.4f} â”‚")
    print(f"â”‚ {'DuyarlÄ±lÄ±k (Recall)':<23} â”‚ {test_recall:<8.4f} â”‚ {np.mean(cv_scores['recall']):<8.4f} â”‚ {test_recall - np.mean(cv_scores['recall']):+8.4f} â”‚")
    print(f"â”‚ {'F1-Score':<23} â”‚ {test_f1:<8.4f} â”‚ {np.mean(cv_scores['f1']):<8.4f} â”‚ {test_f1 - np.mean(cv_scores['f1']):+8.4f} â”‚")
    print(f"â”‚ {'AUC-ROC':<23} â”‚ {test_auc:<8.4f} â”‚ {np.mean(cv_scores['auc']):<8.4f} â”‚ {test_auc - np.mean(cv_scores['auc']):+8.4f} â”‚")
    print(f"â”‚ {'AUC-PR (Avg Precision)':<23} â”‚ {test_auc_pr:<8.4f} â”‚ {'N/A':<8} â”‚ {'â”€':<13} â”‚")
    print(f"â”‚ {'MCC (Matthews Corr.)':<23} â”‚ {test_mcc:<8.4f} â”‚ {'N/A':<8} â”‚ {'â”€':<13} â”‚")
    print(f"â””{'â”€'*25}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*15}â”˜")
    
    print(f"\nğŸš€ NÄ°HAÄ° LSTM-CNN MODEL TEST PERFORMANSI (OPTÄ°MAL EÅÄ°K: {optimal_threshold:.3f}):")
    print(f"â”Œ{'â”€'*25}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*15}â”")
    print(f"â”‚ {'Metrik':<23} â”‚ {'Test':<8} â”‚ {'CV Ort.':<8} â”‚ {'Fark':<13} â”‚")
    print(f"â”œ{'â”€'*25}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*15}â”¤")
    print(f"â”‚ {'DoÄŸruluk (Accuracy)':<23} â”‚ {test_accuracy_opt:<8.4f} â”‚ {np.mean(cv_scores['accuracy_opt']):<8.4f} â”‚ {test_accuracy_opt - np.mean(cv_scores['accuracy_opt']):+8.4f} â”‚")
    print(f"â”‚ {'Kesinlik (Precision)':<23} â”‚ {test_precision_opt:<8.4f} â”‚ {np.mean(cv_scores['precision_opt']):<8.4f} â”‚ {test_precision_opt - np.mean(cv_scores['precision_opt']):+8.4f} â”‚")
    print(f"â”‚ {'DuyarlÄ±lÄ±k (Recall)':<23} â”‚ {test_recall_opt:<8.4f} â”‚ {np.mean(cv_scores['recall_opt']):<8.4f} â”‚ {test_recall_opt - np.mean(cv_scores['recall_opt']):+8.4f} â”‚")
    print(f"â”‚ {'F1-Score':<23} â”‚ {test_f1_opt:<8.4f} â”‚ {np.mean(cv_scores['f1_opt']):<8.4f} â”‚ {test_f1_opt - np.mean(cv_scores['f1_opt']):+8.4f} â”‚")
    print(f"â”‚ {'AUC-ROC':<23} â”‚ {test_auc:<8.4f} â”‚ {np.mean(cv_scores['auc']):<8.4f} â”‚ {test_auc - np.mean(cv_scores['auc']):+8.4f} â”‚")
    print(f"â”‚ {'AUC-PR (Avg Precision)':<23} â”‚ {test_auc_pr:<8.4f} â”‚ {'N/A':<8} â”‚ {'â”€':<13} â”‚")
    print(f"â”‚ {'MCC (Matthews Corr.)':<23} â”‚ {test_mcc_opt:<8.4f} â”‚ {'N/A':<8} â”‚ {'â”€':<13} â”‚")
    print(f"â””{'â”€'*25}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*15}â”˜")
    
    # Optimal eÅŸik iyileÅŸme analizi - TÃ¼m metrikler iÃ§in
    # Entropy Ã¶zeti (Test vs CV)
    if 'entropy' in cv_scores:
        print(f"\nEntropy (Shannon, bit): Test={test_entropy:.4f} | CV Ort={np.mean(cv_scores['entropy']):.4f} | Fark={test_entropy - np.mean(cv_scores['entropy']):+8.4f}")

    test_accuracy_improvement = ((test_accuracy_opt - test_accuracy) / test_accuracy) * 100
    test_precision_improvement = ((test_precision_opt - test_precision) / test_precision) * 100
    test_recall_improvement = ((test_recall_opt - test_recall) / test_recall) * 100
    test_f1_improvement = ((test_f1_opt - test_f1) / test_f1) * 100
    test_mcc_improvement = ((test_mcc_opt - test_mcc) / abs(test_mcc) * 100 if test_mcc != 0 else 0)
    
    print(f"\nâš¡ TEST SETÄ° OPTÄ°MAL EÅÄ°K Ä°YÄ°LEÅME ANALÄ°ZÄ°:")
    print(f"   â€¢ Accuracy Ä°yileÅŸme: %{test_accuracy_improvement:+.2f}")
    print(f"   â€¢ Precision Ä°yileÅŸme: %{test_precision_improvement:+.2f}")
    print(f"   â€¢ Recall Ä°yileÅŸme: %{test_recall_improvement:+.2f}")
    print(f"   â€¢ F1-Score Ä°yileÅŸme: %{test_f1_improvement:+.2f}")
    print(f"   â€¢ MCC Ä°yileÅŸme: %{test_mcc_improvement:+.2f}")
    
    # Confusion Matrix tablosu
    print(f"\nğŸ” TEST SETÄ° CONFUSION MATRIX:")
    print(f"â”Œ{'â”€'*20}â”¬{'â”€'*12}â”¬{'â”€'*12}â”")
    header_text = "Tahmin \\ GerÃ§ek"
    print(f"â”‚ {header_text:<18} â”‚ {'Normal':<10} â”‚ {'ArÄ±zalÄ±':<10} â”‚")
    print(f"â”œ{'â”€'*20}â”¼{'â”€'*12}â”¼{'â”€'*12}â”¤")
    print(f"â”‚ {'ArÄ±za Yok (0)':<18} â”‚ {tn:<10,} â”‚ {fp:<10,} â”‚")
    print(f"â”‚ {'ArÄ±za Var (1)':<18} â”‚ {fn:<10,} â”‚ {tp:<10,} â”‚")
    print(f"â””{'â”€'*20}â”´{'â”€'*12}â”´{'â”€'*12}â”˜")

    # PR eÄŸrisi gÃ¶rseli ve optimal eÅŸik iÅŸaretleme
    try:
        plot_pr_curve(test_results)
    except Exception as _e:
        pass

def plot_training_history_old(test_results):
    """Model eÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtirir (ESKÄ° VERSÄ°YON - kullanÄ±lmÄ±yor)"""
    plt.style.use('default')
    
    # Training history'yi al
    history = test_results['history']
    
    # Training/Validation Loss Grafikleri
    _, (ax1, ax2) = plt.subplots(1, 2, figsize=VisualizationConfig.FIGURE_SIZE_LOSS)
    
    # Loss grafiÄŸi
    epochs_range = range(1, len(history['loss']) + 1)
    ax1.plot(epochs_range, history['loss'], 'b-', linewidth=VisualizationConfig.LINE_WIDTH, label='Training Loss')
    ax1.plot(epochs_range, history['val_loss'], 'r-', linewidth=VisualizationConfig.LINE_WIDTH, label='Validation Loss')
    ax1.set_title('Model Loss Grafikleri', fontweight='bold', fontsize=VisualizationConfig.TITLE_FONT_SIZE, pad=20)
    ax1.set_xlabel('Epoch', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax1.set_ylabel('Loss', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax1.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    ax1.grid(True, alpha=VisualizationConfig.GRID_ALPHA)
    
    # Accuracy grafiÄŸi
    ax2.plot(epochs_range, history['binary_accuracy'], 'b-', linewidth=VisualizationConfig.LINE_WIDTH, label='Training Accuracy')
    ax2.plot(epochs_range, history['val_binary_accuracy'], 'r-', linewidth=VisualizationConfig.LINE_WIDTH, label='Validation Accuracy')
    ax2.set_title('Model Accuracy Grafikleri', fontweight='bold', fontsize=VisualizationConfig.TITLE_FONT_SIZE, pad=20)
    ax2.set_xlabel('Epoch', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax2.set_ylabel('Accuracy', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax2.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    ax2.grid(True, alpha=VisualizationConfig.GRID_ALPHA)
    
    # Early stopping noktasÄ±nÄ± iÅŸaretle
    if len(history['loss']) < TrainingConfig.FINAL_MODEL_EPOCHS:
        stopped_epoch = len(history['loss'])
        ax1.axvline(x=stopped_epoch, color='green', linestyle='--', alpha=0.7, 
                   label=f'Early Stop (Epoch {stopped_epoch})')
        ax2.axvline(x=stopped_epoch, color='green', linestyle='--', alpha=0.7,
                   label=f'Early Stop (Epoch {stopped_epoch})')
        ax1.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
        ax2.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    
    plt.tight_layout()
    save_path = METRICS_DIR / 'training_history_old.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Eski format eÄŸitim grafiÄŸi kaydedildi: {save_path}")
    plt.show()

def plot_fold_performance(cv_scores):
    """Fold bazÄ±nda performans trendini gÃ¶rselleÅŸtirir"""
    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_FOLD_PERFORMANCE)
    folds = range(1, TrainingConfig.CV_SPLITS + 1)
    
    colors = VisualizationConfig.COLORS
    ax.plot(folds, cv_scores['accuracy'], 'o-', label='Accuracy', linewidth=VisualizationConfig.LINE_WIDTH, 
            markersize=VisualizationConfig.MARKER_SIZE, color=colors['accuracy'])
    ax.plot(folds, cv_scores['f1'], 's-', label='F1-Score', linewidth=VisualizationConfig.LINE_WIDTH, 
            markersize=VisualizationConfig.MARKER_SIZE, color=colors['f1'])
    ax.plot(folds, cv_scores['auc'], '^-', label='AUC', linewidth=VisualizationConfig.LINE_WIDTH, 
            markersize=VisualizationConfig.MARKER_SIZE, color=colors['auc'])
    ax.plot(folds, cv_scores['precision'], 'd-', label='Precision', linewidth=VisualizationConfig.LINE_WIDTH, 
            markersize=VisualizationConfig.MARKER_SIZE, color=colors['precision'])
    ax.plot(folds, cv_scores['recall'], 'v-', label='Recall', linewidth=VisualizationConfig.LINE_WIDTH, 
            markersize=VisualizationConfig.MARKER_SIZE, color=colors['recall'])
    
    ax.set_title('Fold BazÄ±nda Performans Trendi', fontweight='bold', fontsize=VisualizationConfig.TITLE_FONT_SIZE, pad=20)
    ax.set_xlabel('Fold NumarasÄ±', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax.set_ylabel('Skor', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE, loc='best')
    ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA)
    ax.set_xticks(folds)
    ax.set_ylim(0, 1)
    
    plt.tight_layout()
    save_path = METRICS_DIR / 'fold_performance.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Fold performans grafiÄŸi kaydedildi: {save_path}")
    plt.show()

def plot_confusion_matrix(test_results):
    """Test seti confusion matrix'ini gÃ¶rselleÅŸtirir"""
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    y_pred = (y_pred_prob > TrainingConfig.DEFAULT_THRESHOLD).astype(int)  # Config'ten varsayÄ±lan eÅŸik
    
    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_CONFUSION_MATRIX)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    cm_display = np.array([[tn, fp], [fn, tp]])
    
    # HÃ¼cre aÃ§Ä±klamalarÄ± iÃ§in Ã¶zel annotasyon 
    cell_labels = np.array([
        [f'{tn}\n(True Negative)', 
         f'{fp}\n(False Positive)'],
        [f'{fn}\n(False Negative)', 
         f'{tp}\n(True Positive)']
    ])
    
    sns.heatmap(cm_display, annot=cell_labels, fmt='', cmap='Blues', 
                xticklabels=['ArÄ±za Yok (0)', 'ArÄ±za Var (1)'], 
                yticklabels=['ArÄ±za Yok (0)', 'ArÄ±za Var (1)'], ax=ax, 
                cbar_kws={'label': 'SayÄ±'}, annot_kws={'size': 12, 'weight': 'bold'})
    ax.set_title('Test Seti Confusion Matrix', fontweight='bold', fontsize=VisualizationConfig.TITLE_FONT_SIZE, pad=20)
    # scikit-learn confusion_matrix: satÄ±rlar gerÃ§ek (y), sÃ¼tunlar tahmin (y^)
    ax.set_xlabel('Tahmin Edilen SÄ±nÄ±f', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax.set_ylabel('GerÃ§ek SÄ±nÄ±f', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    
    plt.tight_layout()
    save_path = METRICS_DIR / 'confusion_matrix.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Confusion matrix grafiÄŸi kaydedildi: {save_path}")
    plt.show()

def plot_pr_curve(test_results):
    """Precision-Recall eÄŸrisini Ã§izer ve optimal eÅŸiÄŸi iÅŸaretler."""
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    t_opt = float(test_results['optimal_threshold'])

    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_prob)
    # thresholds uzunluÄŸu = len(precisions) - 1 olduÄŸundan hizalama iÃ§in [1:] kullanÄ±lÄ±r
    prec = precisions[1:]
    rec = recalls[1:]

    # En yakÄ±n eÅŸiÄŸi bul ve ilgili (P,R) noktasÄ±nÄ± iÅŸaretle
    if thresholds.size > 0:
        idx = int(np.argmin(np.abs(thresholds - t_opt)))
        p_sel = float(prec[idx])
        r_sel = float(rec[idx])
    else:
        p_sel, r_sel = float(precisions[-1]), float(recalls[-1])

    ap = average_precision_score(y_test, y_pred_prob)

    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_PR_CURVE)
    colors = VisualizationConfig.COLORS
    ax.plot(recalls, precisions, color=colors.get('pr_curve', '#E67E22'),
            linewidth=VisualizationConfig.LINE_WIDTH, label=f'PR EÄŸrisi (AP = {ap:.3f})')

    # Optimal eÅŸiÄŸi iÅŸaretle
    ax.scatter([r_sel], [p_sel], color='red', s=60, zorder=5,
               label=f'Optimal EÅŸik = {t_opt:.3f}\n(P={p_sel:.2f}, R={r_sel:.2f})')

    # Baseline (no-skill): pozitif prevalansÄ±
    pos_rate = np.mean(np.asarray(y_test).ravel())
    ax.hlines(pos_rate, 0, 1, colors=colors.get('random_line', '#95A5A6'), linestyles='--',
              label=f'No-skill (Pos Rate={pos_rate:.2f})')

    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_xlabel('Recall', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_ylabel('Precision', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_title('Precision-Recall EÄŸrisi', fontsize=VisualizationConfig.TITLE_FONT_SIZE, fontweight='bold')
    ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA)
    ax.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE, loc='best')

    plt.tight_layout()
    save_path = METRICS_DIR / 'precision_recall_curve.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… PR eÄŸrisi kaydedildi: {save_path}")
    plt.show()

def plot_cv_vs_test_comparison(test_results, cv_scores):
    """CV vs Test performans karÅŸÄ±laÅŸtÄ±rmasÄ±nÄ± gÃ¶rselleÅŸtirir"""
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    y_pred = (y_pred_prob > TrainingConfig.DEFAULT_THRESHOLD).astype(int)  # Config'ten varsayÄ±lan eÅŸik
    
    # Test skorlarÄ± hesapla
    test_accuracy = accuracy_score(y_test, y_pred)
    test_precision = precision_score(y_test, y_pred, zero_division=0)
    test_recall = recall_score(y_test, y_pred, zero_division=0)
    test_f1 = f1_score(y_test, y_pred, zero_division=0)
    test_auc = roc_auc_score(y_test, y_pred_prob)
    
    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_CV_TEST_COMPARISON)
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
    cv_means = [np.mean(cv_scores['accuracy']), np.mean(cv_scores['precision']), 
                np.mean(cv_scores['recall']), np.mean(cv_scores['f1']), np.mean(cv_scores['auc'])]
    test_scores = [test_accuracy, test_precision, test_recall, test_f1, test_auc]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    colors = VisualizationConfig.COLORS
    ax.bar(x - width/2, cv_means, width, label='CV Ortalama', alpha=0.8, color=colors['cv_mean'])
    ax.bar(x + width/2, test_scores, width, label='Test Sonucu', alpha=0.8, color=colors['test_result'])
    
    # DeÄŸerleri bar'larÄ±n Ã¼stÃ¼ne yaz
    for i, (cv_val, test_val) in enumerate(zip(cv_means, test_scores)):
        ax.text(i - width/2, cv_val + 0.01, f'{cv_val:.3f}', ha='center', va='bottom', fontweight='bold')
        ax.text(i + width/2, test_val + 0.01, f'{test_val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    ax.set_title('Cross Validation vs Test PerformansÄ±', fontweight='bold', fontsize=VisualizationConfig.TITLE_FONT_SIZE, pad=20)
    ax.set_xlabel('Metrikler', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax.set_ylabel('Skor', fontweight='bold', fontsize=VisualizationConfig.LABEL_FONT_SIZE)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA, axis='y')
    ax.set_ylim(0, 1)
    
    plt.tight_layout()
    save_path = METRICS_DIR / 'cv_vs_test_comparison.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ CV vs Test karÅŸÄ±laÅŸtÄ±rma grafiÄŸi kaydedildi: {save_path}")
    plt.show()

def plot_roc_curve(test_results):
    """ROC eÄŸrisini gÃ¶rselleÅŸtirir"""
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    
    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_ROC_CURVE)
    
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    test_auc = roc_auc_score(y_test, y_pred_prob)
    
    colors = VisualizationConfig.COLORS
    ax.plot(fpr, tpr, color=colors['roc_curve'], linewidth=VisualizationConfig.LINE_WIDTH, 
            label=f'ROC EÄŸrisi (AUC = {test_auc:.3f})')
    ax.plot([0, 1], [0, 1], color=colors['random_line'], linestyle='--', linewidth=2, 
            label='Rastgele SÄ±nÄ±flandÄ±rma')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_ylabel('True Positive Rate', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_title('ROC Curve', fontsize=VisualizationConfig.TITLE_FONT_SIZE, fontweight='bold')
    ax.legend(loc="lower right", fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA)

    plt.tight_layout()
    save_path = METRICS_DIR / 'roc_curve.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ ROC Curve grafiÄŸi kaydedildi: {save_path}")
    plt.show()

def plot_precision_recall_curve(test_results):
    """Precision-Recall eÄŸrisini gÃ¶rselleÅŸtirir.
    
    Dengesiz veri setleri iÃ§in ROC eÄŸrisinden daha bilgilendirici olan
    Precision-Recall eÄŸrisini Ã§izer. Ã–zellikle azÄ±nlÄ±k sÄ±nÄ±fÄ±nÄ±n (arÄ±zalÄ±
    makineler) ne kadar iyi tespit edildiÄŸini gÃ¶sterir.

    Args:
        test_results (dict): Test sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k:
            - y_test: GerÃ§ek test etiketleri
            - y_pred_prob: Tahmin olasÄ±lÄ±klarÄ±
            - dataset_name: Dataset adÄ± (opsiyonel)

    Example:
        >>> test_results = {'y_test': y_test, 'y_pred_prob': y_pred_prob}
        >>> plot_precision_recall_curve(test_results)
        # PR eÄŸrisi ve AUC-PR deÄŸeri ile grafik Ã§izilir
    """
    y_test = test_results['y_true']
    y_pred_prob = test_results['y_pred_proba']
    
    _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_PR_CURVE)
    
    # PR eÄŸrisi hesapla
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)
    pr_auc = average_precision_score(y_test, y_pred_prob)
    
    # Pozitif sÄ±nÄ±f oranÄ±nÄ± hesapla (baseline iÃ§in)
    pos_ratio = sum(y_test) / len(y_test)
    
    colors = VisualizationConfig.COLORS
    ax.plot(recall, precision, color=colors['pr_curve'], linewidth=VisualizationConfig.LINE_WIDTH, 
            label=f'PR EÄŸrisi (AUC-PR = {pr_auc:.3f})')
    ax.axhline(y=pos_ratio, color=colors['random_line'], linestyle='--', linewidth=2, 
               label=f'Rastgele SÄ±nÄ±flandÄ±rma (AP = {pos_ratio:.3f})')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall (DuyarlÄ±lÄ±k)', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_ylabel('Precision (Kesinlik)', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
    ax.set_title('Precision-Recall Curve', fontsize=VisualizationConfig.TITLE_FONT_SIZE, fontweight='bold')
    ax.legend(loc="lower left", fontsize=VisualizationConfig.LEGEND_FONT_SIZE)
    ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA)

    plt.tight_layout()
    save_path = METRICS_DIR / 'precision_recall_curve.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ Precision-Recall Curve grafiÄŸi kaydedildi: {save_path}")
    plt.show()

# reporting.py dosyasÄ±nÄ±n sonuna eklenecek yeni fonksiyon

def raporla_performans_olcutleri(confusion_matrix):
    """
    Verilen bir 2x2 confusion matrix'ten tÃ¼m performans Ã¶lÃ§Ã¼tlerini hesaplar
    ve formatlanmÄ±ÅŸ bir metin tablosu olarak dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        tn, fp, fn, tp = confusion_matrix.ravel()

        # Metrik HesaplamalarÄ±
        total = tp + tn + fp + fn
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0
        mcc_numerator = (tp * tn) - (fp * fn)
        mcc_denominator = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        mcc = mcc_numerator / mcc_denominator if mcc_denominator > 0 else 0

        results = {
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall (TPR)": recall,
            "Specificity (TNR)": specificity,
            "F1-Score": f1,
            "FPR": fpr,
            "FNR": fnr,
            "Matthews Corr. Coef. (MCC)": mcc
        }
        
        # Tablo metnini oluÅŸtur
        table = "\n" + "="*50 + "\n"
        table += "ğŸ“Š PERFORMANS Ã–LÃ‡ÃœTLERÄ° RAPORU (TEST SETÄ°) ğŸ“Š\n"
        table += "="*50 + "\n"
        table += f"â”Œ{'â”€'*32}â”¬{'â”€'*15}â”\n"
        table += f"â”‚ {'Metrik':<30} â”‚ {'DeÄŸer':<13} â”‚\n"
        table += f"â”œ{'â”€'*32}â”¼{'â”€'*15}â”¤\n"
        for name, value in results.items():
            table += f"â”‚ {name:<30} â”‚ {value:<13.4f} â”‚\n"
        table += f"â””{'â”€'*32}â”´{'â”€'*15}â”˜\n"
        
        return table

    except Exception as e:
        return f"\nâŒ Performans metrikleri hesaplanÄ±rken bir hata oluÅŸtu: {e}"

def plot_training_history(history, save_path='training_history.png'):
    """
    Model eÄŸitim geÃ§miÅŸini (loss ve accuracy) gÃ¶rselleÅŸtirir.
    Training ve Validation loss'larÄ±nÄ± karÅŸÄ±laÅŸtÄ±rarak overfitting analizi yapar.
    
    Args:
        history (dict): model.fit() tarafÄ±ndan dÃ¶ndÃ¼rÃ¼len history.history
        save_path (str): GrafiÄŸin kaydedileceÄŸi dosya yolu
    """
    try:
        # History'den metrikleri al
        train_loss = history.get('loss', [])
        val_loss = history.get('val_loss', [])
        # Accuracy yerine PR AUC'i gÃ¶ster (mevcutsa); yoksa eski accuracy'ye dÃ¼ÅŸ
        train_acc = history.get('pr_auc', []) or history.get('binary_accuracy', [])
        val_acc = history.get('val_pr_auc', []) or history.get('val_binary_accuracy', [])
        
        if not train_loss or not val_loss:
            print("âš ï¸ Loss deÄŸerleri bulunamadÄ±, grafik Ã§izilemiyor.")
            return
        
        epochs = range(1, len(train_loss) + 1)
        
        # Loss farkÄ±nÄ± hesapla (overfitting analizi iÃ§in)
        final_train_loss = train_loss[-1]
        final_val_loss = val_loss[-1]
        loss_diff = final_val_loss - final_train_loss
        loss_diff_percent = (loss_diff / final_train_loss) * 100 if final_train_loss > 0 else 0
        
        # Overfitting durumu belirleme
        if loss_diff < 0.01:
            overfitting_status = "âœ… MÃœKEMMEL! SaÄŸlÄ±klÄ± eÄŸitim"
            status_color = 'green'
        elif loss_diff < 0.03:
            overfitting_status = "âœ… Ä°YÄ°! Kabul edilebilir"
            status_color = 'blue'
        elif loss_diff < 0.05:
            overfitting_status = "âš ï¸ DÄ°KKAT! Orta seviye overfitting"
            status_color = 'orange'
        elif loss_diff < 0.10:
            overfitting_status = "âŒ SORUN! YÃ¼ksek overfitting"
            status_color = 'red'
        else:
            overfitting_status = "âŒâŒ CÄ°DDÄ° SORUN! Ã‡ok yÃ¼ksek overfitting"
            status_color = 'darkred'
        
        # 2x1 subplot oluÅŸtur
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # --- LOSS GRAFÄ°ÄÄ° ---
        ax1.plot(epochs, train_loss, 'b-', linewidth=2, label='Training Loss', marker='o', markersize=3)
        ax1.plot(epochs, val_loss, 'r-', linewidth=2, label='Validation Loss', marker='s', markersize=3)
        
        # Son deÄŸerleri vurgula
        ax1.scatter(epochs[-1], train_loss[-1], color='blue', s=200, zorder=5, 
                   edgecolors='black', linewidths=2)
        ax1.scatter(epochs[-1], val_loss[-1], color='red', s=200, zorder=5, 
                   edgecolors='black', linewidths=2)
        
        # Son deÄŸerleri gÃ¶ster
        ax1.text(epochs[-1], train_loss[-1], f' {train_loss[-1]:.4f}', 
                fontsize=10, va='center', ha='left', fontweight='bold', color='blue')
        ax1.text(epochs[-1], val_loss[-1], f' {val_loss[-1]:.4f}', 
                fontsize=10, va='center', ha='left', fontweight='bold', color='red')
        
        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
        ax1.set_title('ğŸ“‰ Training vs Validation Loss', fontsize=14, fontweight='bold')
        ax1.legend(loc='upper right', fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Overfitting analizi metni ekle
        textstr = f'Son Training Loss: {final_train_loss:.4f}\n'
        textstr += f'Son Validation Loss: {final_val_loss:.4f}\n'
        textstr += f'Fark: {loss_diff:.4f} ({loss_diff_percent:+.2f}%)\n'
        textstr += f'\n{overfitting_status}'
        
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
        ax1.text(0.02, 0.98, textstr, transform=ax1.transAxes, fontsize=9,
                verticalalignment='top', bbox=props, family='monospace')
        
        # --- ACCURACY GRAFÄ°ÄÄ° ---
        if train_acc and val_acc:
            ax2.plot(epochs, train_acc, 'b-', linewidth=2, label='Training PR AUC', marker='o', markersize=3)
            ax2.plot(epochs, val_acc, 'r-', linewidth=2, label='Validation PR AUC', marker='s', markersize=3)
            
            # Son deÄŸerleri vurgula
            ax2.scatter(epochs[-1], train_acc[-1], color='blue', s=200, zorder=5, 
                       edgecolors='black', linewidths=2)
            ax2.scatter(epochs[-1], val_acc[-1], color='red', s=200, zorder=5, 
                       edgecolors='black', linewidths=2)
            
            # Son deÄŸerleri gÃ¶ster
            ax2.text(epochs[-1], train_acc[-1], f' {train_acc[-1]:.4f}', 
                    fontsize=10, va='center', ha='left', fontweight='bold', color='blue')
            ax2.text(epochs[-1], val_acc[-1], f' {val_acc[-1]:.4f}', 
                    fontsize=10, va='center', ha='left', fontweight='bold', color='red')
            
            ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
            ax2.set_ylabel('PR AUC', fontsize=12, fontweight='bold')
            ax2.set_title('ğŸ“ˆ Training vs Validation PR AUC', fontsize=14, fontweight='bold')
            ax2.legend(loc='lower right', fontsize=10)
            ax2.grid(True, alpha=0.3)
            
            # Accuracy farkÄ± analizi
            final_train_acc = train_acc[-1]
            final_val_acc = val_acc[-1]
            acc_diff = final_train_acc - final_val_acc
            acc_diff_percent = (acc_diff / final_train_acc) * 100 if final_train_acc > 0 else 0
            
            acc_textstr = f'Son Training Acc: {final_train_acc:.4f}\n'
            acc_textstr += f'Son Validation Acc: {final_val_acc:.4f}\n'
            acc_textstr += f'Fark: {acc_diff:.4f} ({acc_diff_percent:+.2f}%)'
            
            ax2.text(0.02, 0.02, acc_textstr, transform=ax2.transAxes, fontsize=9,
                    verticalalignment='bottom', bbox=props, family='monospace')
        
        plt.suptitle('ğŸ“ MODEL EÄÄ°TÄ°M GEÃ‡MÄ°ÅÄ° VE OVERFÄ°TTÄ°NG ANALÄ°ZÄ°', 
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        # Dosyaya kaydet
        full_save_path = METRICS_DIR / save_path
        plt.savefig(full_save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ EÄŸitim geÃ§miÅŸi grafiÄŸi kaydedildi: {full_save_path}")
        
        # Konsola Ã¶zet yazdÄ±r
        print(f"\n{'='*70}")
        print(f"ğŸ“Š EÄÄ°TÄ°M GEÃ‡MÄ°ÅÄ° ANALÄ°ZÄ°")
        print(f"{'='*70}")
        print(f"ğŸ“‰ LOSS ANALÄ°ZÄ°:")
        print(f"   â€¢ Son Training Loss:    {final_train_loss:.6f}")
        print(f"   â€¢ Son Validation Loss:  {final_val_loss:.6f}")
        print(f"   â€¢ Loss FarkÄ±:           {loss_diff:.6f} ({loss_diff_percent:+.2f}%)")
        print(f"   â€¢ Durum:                {overfitting_status}")
        
        if train_acc and val_acc:
            print(f"\nğŸ“ˆ PR AUC ANALÄ°ZÄ°:")
            print(f"   â€¢ Son Training PR AUC:  {final_train_acc:.6f}")
            print(f"   â€¢ Son Validation PR AUC:{final_val_acc:.6f}")
            print(f"   â€¢ PR AUC FarkÄ±:         {acc_diff:.6f} ({acc_diff_percent:+.2f}%)")
        
        print(f"\nğŸ¯ REGÃœLARÄ°ZASYON DEÄERLENDÄ°RMESÄ°:")
        if abs(loss_diff) < 0.02:
            print(f"   âœ… Dropout ve regÃ¼lasyonlar mÃ¼kemmel Ã§alÄ±ÅŸÄ±yor!")
            print(f"   âœ… Model saÄŸlÄ±klÄ± ÅŸekilde Ã¶ÄŸreniyor (generalize ediyor)")
        elif abs(loss_diff) < 0.05:
            print(f"   âœ… RegÃ¼lasyonlar iyi Ã§alÄ±ÅŸÄ±yor, hafif overfitting var")
            print(f"   â„¹ï¸ Normal kabul edilebilir bir seviye")
        else:
            print(f"   âš ï¸ Overfitting tespit edildi!")
            print(f"   ğŸ’¡ Ã–neriler:")
            print(f"      â€¢ Dropout oranlarÄ±nÄ± artÄ±rÄ±n (DROPOUT_RATES)")
            print(f"      â€¢ L2 regularization ekleyin/artÄ±rÄ±n")
            print(f"      â€¢ Early stopping patience'Ä± azaltÄ±n")
            print(f"      â€¢ Model kapasitesini azaltÄ±n (daha az nÃ¶ron)")
            print(f"      â€¢ Daha fazla eÄŸitim verisi toplayÄ±n")
        
        print(f"{'='*70}\n")
        
        plt.show()
        
    except Exception as e:
        print(f"âŒ EÄŸitim geÃ§miÅŸi grafiÄŸi oluÅŸturulurken hata: {e}")
        import traceback
        traceback.print_exc()


def plot_performans_olcutleri(confusion_matrix):
    """
    Performans Ã¶lÃ§Ã¼tlerini gÃ¶rselleÅŸtirir - bar chart olarak gÃ¶sterir.
    """
    try:
        import math
        tn, fp, fn, tp = confusion_matrix.ravel()

        # Metrik HesaplamalarÄ±
        total = tp + tn + fp + fn
        accuracy = (tp + tn) / total if total > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0
        mcc_numerator = (tp * tn) - (fp * fn)
        mcc_denominator = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        mcc = mcc_numerator / mcc_denominator if mcc_denominator > 0 else 0

        # Ana metrikler (0-1 arasÄ±nda)
        main_metrics = {
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "Specificity": specificity,
            "F1-Score": f1
        }
        
        # Hata oranlarÄ±
        error_metrics = {
            "FPR": fpr,
            "FNR": fnr
        }

        # Ä°ki ayrÄ± grafik oluÅŸtur
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Ana metrikler grafiÄŸi
        metrics_names = list(main_metrics.keys())
        metrics_values = list(main_metrics.values())
        
        colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12', '#9b59b6']
        bars1 = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.8)
        
        # DeÄŸerleri bar'larÄ±n Ã¼stÃ¼ne yaz
        for bar, value in zip(bars1, metrics_values):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax1.set_title('Ana Performans Metrikleri', fontsize=14, fontweight='bold', pad=20)
        ax1.set_ylabel('DeÄŸer', fontsize=12, fontweight='bold')
        ax1.set_ylim(0, 1.1)
        ax1.grid(True, alpha=0.3, axis='y')
        ax1.tick_params(axis='x', rotation=45)
        
        # Hata oranlarÄ± grafiÄŸi
        error_names = list(error_metrics.keys())
        error_values = list(error_metrics.values())
        
        bars2 = ax2.bar(error_names, error_values, color=['#e74c3c', '#e67e22'], alpha=0.8)
        
        # DeÄŸerleri bar'larÄ±n Ã¼stÃ¼ne yaz
        for bar, value in zip(bars2, error_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax2.set_title('Hata OranlarÄ±', fontsize=14, fontweight='bold', pad=20)
        ax2.set_ylabel('Oran', fontsize=12, fontweight='bold')
        ax2.set_ylim(0, max(max(error_values), 0.1) * 1.2)
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.tick_params(axis='x', rotation=45)
        
        # MCC deÄŸerini ayrÄ± bir text box olarak ekle
        textstr = f'Matthews Correlation Coefficient (MCC): {mcc:.4f}'
        props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)
        fig.text(0.5, 0.02, textstr, transform=fig.transFigure, fontsize=12,
                verticalalignment='bottom', horizontalalignment='center', bbox=props)
        
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15)  # MCC iÃ§in yer bÄ±rak
        save_path = METRICS_DIR / 'performance_metrics.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ Performans metrikleri grafiÄŸi kaydedildi: {save_path}")
        plt.show()
        
        print("âœ… Performans Ã¶lÃ§Ã¼tleri gÃ¶rselleÅŸtirildi!")

    except Exception as e:
        print(f"âŒ Performans Ã¶lÃ§Ã¼tleri gÃ¶rselleÅŸtirme hatasÄ±: {e}")

def plot_entropy_histogram(test_results, high_entropy_threshold: float = 0.8):
    """Tahmin entropisi (Shannon, bit) daÄŸÄ±lÄ±mÄ±nÄ± histogram olarak Ã§izer.

    Args:
        test_results (dict): 'y_pred_proba' alanÄ±nÄ± iÃ§ermelidir.
        high_entropy_threshold (float): YÃ¼ksek belirsizlik eÅŸiÄŸi (varsayÄ±lan 0.8 bit).
    """
    try:
        y_pred_proba = test_results.get('y_pred_proba') if isinstance(test_results, dict) else None
        if y_pred_proba is None:
            print("Bilgi: test_results['y_pred_proba'] yok; entropi histogramÄ± atlandÄ±.")
            return

        p = np.clip(np.array(y_pred_proba).flatten(), 1e-12, 1 - 1e-12)
        ent = -(p * np.log2(p) + (1 - p) * np.log2(1 - p))
        ent_mean = float(np.mean(ent))
        p25, p50, p75 = [float(x) for x in np.percentile(ent, [25, 50, 75])]
        frac_high = float(np.mean(ent >= high_entropy_threshold))

        _, ax = plt.subplots(figsize=VisualizationConfig.FIGURE_SIZE_PR_CURVE)
        ax.hist(ent, bins=30, color='#3498db', alpha=0.85, edgecolor='white')
        ax.axvline(ent_mean, color='red', linestyle='--', linewidth=2, label=f'Ort: {ent_mean:.3f}')
        ax.axvline(high_entropy_threshold, color='orange', linestyle=':', linewidth=2, label=f'EÅŸik: {high_entropy_threshold}')
        ax.set_title('Tahmin Entropisi DaÄŸÄ±lÄ±mÄ±', fontsize=VisualizationConfig.TITLE_FONT_SIZE, fontweight='bold')
        ax.set_xlabel('Entropy (bit)', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
        ax.set_ylabel('Frekans', fontsize=VisualizationConfig.LABEL_FONT_SIZE, fontweight='bold')
        ax.grid(True, alpha=VisualizationConfig.GRID_ALPHA)
        ax.legend(fontsize=VisualizationConfig.LEGEND_FONT_SIZE, loc='best')
        plt.tight_layout()
        save_path = METRICS_DIR / 'entropy_histogram.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ Entropi histogram grafiÄŸi kaydedildi: {save_path}")
        plt.show()
        print(f"YÃ¼ksek entropi oranÄ± (>{high_entropy_threshold} bit): {frac_high:.2%}")
        print(f"Entropy yÃ¼zdelikleri (bit): p25={p25:.3f}, p50={p50:.3f}, p75={p75:.3f}")
    except Exception as e:
        print(f"Entropi histogramÄ± oluÅŸturulurken hata: {e}")

def print_high_entropy_samples(test_results, top_n: int = None, threshold: float = None):
    """En yÃ¼ksek entropili Ã¶rnekleri tablo halinde yazdÄ±rÄ±r.

    Args:
        test_results (dict): 'y_pred_proba' ve opsiyonel 'y_true' iÃ§erebilir.
        top_n (int): Listelenecek Ã¶rnek sayÄ±sÄ±. None ise config'ten alÄ±nÄ±r.
        threshold (float): YÃ¼ksek entropi eÅŸiÄŸi. None ise config'ten alÄ±nÄ±r.
    """
    try:
        y_pred_proba = test_results.get('y_pred_proba') if isinstance(test_results, dict) else None
        y_true = test_results.get('y_true') if isinstance(test_results, dict) else None
        if y_pred_proba is None:
            print("Bilgi: test_results['y_pred_proba'] yok; yÃ¼ksek entropi listesi atlandÄ±.")
            return

        if top_n is None or threshold is None:
            try:
                from config import VisualizationConfig
                if top_n is None:
                    top_n = getattr(VisualizationConfig, 'ENTROPY_TOP_N', 10)
                if threshold is None:
                    threshold = getattr(VisualizationConfig, 'ENTROPY_HIGH_THRESHOLD', 0.8)
            except Exception:
                top_n = top_n or 10
                threshold = threshold or 0.8

        p = np.clip(np.array(y_pred_proba).flatten(), 1e-12, 1 - 1e-12)
        ent = -(p * np.log2(p) + (1 - p) * np.log2(1 - p))
        idx_sorted = np.argsort(-ent)
        top_idx = idx_sorted[:top_n]

        print("\nYÃ¼ksek Entropili Ã–rnekler (en belirsiz tahminler):")
        header = f"{'Idx':>6}  {'Prob':>8}  {'Entropy(bit)':>14}  {'Y_true':>7}"
        print(header)
        print('-' * len(header))
        for i in top_idx:
            prob = float(p[i])
            ent_i = float(ent[i])
            y_t = int(y_true[i]) if y_true is not None else -1
            print(f"{i:6d}  {prob:8.4f}  {ent_i:14.4f}  {y_t:7d}")

        above = int(np.sum(ent >= threshold))
        total = len(ent)
        print(f"Toplam {above}/{total} Ã¶rnek > {threshold} bit (oran: {above/total:.2%})")
    except Exception as e:
        print(f"YÃ¼ksek entropili Ã¶rnekler listelenirken hata: {e}")

def plot_all_results(cv_scores, test_results):
    """Model performansÄ±nÄ±n tÃ¼m gÃ¶rselleÅŸtirmelerini kapsamlÄ± olarak oluÅŸturur.

    CV ve test sonuÃ§larÄ±nÄ± gÃ¶rsel olarak analiz etmek iÃ§in gerekli tÃ¼m grafikleri
    sÄ±rasÄ±yla Ã§izer. Training history, fold performanslarÄ±, confusion matrix,
    CV vs test karÅŸÄ±laÅŸtÄ±rmasÄ± ve ROC curve grafiklerini iÃ§erir.

    Args:
        cv_scores (dict): Cross validation sonuÃ§larÄ±:
            - individual_scores (list): Her fold iÃ§in detaylÄ± metrikler
            - mean_* (float): Ortalama performans metrikleri
        test_results (dict): Test sonuÃ§larÄ±:
            - history (tf.keras.History): EÄŸitim geÃ§miÅŸi
            - y_test, y_pred, y_pred_prob (array): Test verileri ve tahminler

    Note:
        Her grafik ayrÄ± pencerede aÃ§Ä±lÄ±r. VisualizationConfig'ten renk ve 
        boyut ayarlarÄ± kullanÄ±lÄ±r.

    Example:
        >>> plot_all_results(cv_scores, test_results)
        ğŸ“ˆ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...
        ğŸ‹ï¸â€â™‚ï¸ Training History GrafiÄŸi
        ğŸ“Š Fold BazÄ±nda Performans GrafiÄŸi
        ğŸ¯ Confusion Matrix
        ...
    """
    print(f"\nğŸ“ˆ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
    
    # 1. CV sonuÃ§larÄ± yazdÄ±r
    if cv_scores:
        print_cv_results(cv_scores)
    
    # 2. Test sonuÃ§larÄ± yazdÄ±r
    if test_results:
        print_test_results(test_results, cv_scores)
    
    # 3. Training history - YENÄ° DETAYLI VERSÄ°YON
    if test_results and 'history' in test_results:
        print("\nğŸ‹ï¸â€â™‚ï¸ Training History GrafiÄŸi oluÅŸturuluyor...")
        plot_training_history(test_results['history'], save_path='training_history.png')
    
    # 4. Fold bazÄ±nda performans
    plot_fold_performance(cv_scores)
    
    # 5. Confusion Matrix
    plot_confusion_matrix(test_results)
    
    # 6. DetaylÄ± performans metrikleri raporu
    if test_results and 'y_true' in test_results and 'y_pred' in test_results:
        import math
        cm = confusion_matrix(test_results['y_true'], test_results['y_pred'])
        metrics_table = raporla_performans_olcutleri(cm)
        print(metrics_table)
        
        # 6b. Performans Ã¶lÃ§Ã¼tlerini gÃ¶rselleÅŸtir
        plot_performans_olcutleri(cm)
    
    # 7. CV vs Test karÅŸÄ±laÅŸtÄ±rmasÄ±
    plot_cv_vs_test_comparison(test_results, cv_scores)
    
    # 8. ROC Curve
    plot_roc_curve(test_results)
    
    # 9. Precision-Recall Curve
    plot_precision_recall_curve(test_results)
    
    # 10. Entropy Histogram
    plot_entropy_histogram(test_results, VisualizationConfig.ENTROPY_HIGH_THRESHOLD)
    
    # 11. YÃ¼ksek Entropili Ã–rnekler
    print_high_entropy_samples(test_results)
    
    print(f"âœ… TÃ¼m gÃ¶rselleÅŸtirmeler tamamlandÄ±!") 
